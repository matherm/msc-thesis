\chapter{Zusammenfassung}
%- Fazit 												( 3 Seiten)	( 3 Seiten)	
Das Feld von Deep Learning und Convolutional Neural Networks (CNNs) fasst sehr viele interessante Facetten. In dieser Arbeit wurden die Bereiche des überwachten neuronalen Lernmodells zur Klassifikation und Regression aufbereitet. Aufbauend auf das klassische Multilayer Perceptron (MLP), wurden die speziellen Convolutional Neural Networks eingeführt. Diese lassen sich wie gewohnte mehrschichtige Netze mittels Backpropagation-Algorithmus trainieren, bieten jedoch aufgrund ihrer Architektur große Vorteile gegenüber MLPs. Hierbei ist besonders deren Eigenschaft hervorzuheben, lokale Merkmale im Eingaberaum zu berücksichtigen. Darüber hinaus stellt die Entkopplung der Dimensionalität der Eingabe von der Anzahl der trainierbaren Gewichte einen weiteren Vorteil der Convolutional Neural Networks dar. In den darauffolgenden Kapiteln wurde aufgezeigt, warum tiefe Netze schwer zu trainieren sind und wie moderne Methoden im Bereich Deep Learning dabei helfen können, diese Schwierigkeiten zu überwinden. Als die beiden wichtigsten Beispiele wurden das \textit{Overfitting} sowie der \textit{Vanishing Gradient}-Effekt vorgestellt. Als Einstieg in das Thema seien an dieser Stelle die Arbeiten von \cite{Bengio2009} und \cite{Bengio2012} empfohlen. Im weiteren Verlauf der Arbeit wurde, basierend auf den vorgestellten Methoden, ein eigenes CNN implementiert und mit diversen Experimenten auf Richtigkeit überprüft. 
In diesem Kapitel werden die Ergebnisse nochmals zusammengefasst und ein Ausblick auf weitere mögliche Forschungstätigkeit im Umfeld von CNNs gegeben.
							

\section{Ergebnisse}
Der im Rahmen dieser Arbeit entwickelte CPU-basierte Prototyp eines CNN konnte durch die Experimente bereits seine Leistungsfähigkeit unter Beweis stellen. Neben den aktuellen Methoden im Deep Learning zur Initialisierung und Optimierung, sind auch nützliche Methoden zur Visualisierung sowie zum unüberwachten Vortraining implementiert worden. Neben einer Python-Schnittstelle weist das \textit{ConvNetCPP} wenige Abhängigkeiten auf und kann so leicht auf anderen Systemen eingesetzt werden.

Für die Anwendung ist als Vorverarbeitungsstufe die Zentrierung der Daten besonders wichtig, da ansonsten die Logistische Sigmoidfunktion oder der Tangens Hyperbolicus schnell sättigen können. Auch der HSV-Farbraum liefert konsistent etwas bessere Ergebnisse als der RGB-Farbraum. Im Rahmen der Initialisierung kann gefolgert werden, dass die Initialisierung der ersten Schicht durch unüberwachtes Vortraining nicht ausreicht, um die schlechte Initialisierung des restlichen Netzes zu reparieren. Ebenso scheint die Standardabweichung von $\sigma = 0.01$ der Standard-Initialisierung für das CIFAR-10-Problem zu klein zu sein. Dies zeigt auch das Experiment, bei dem ein sonst Xavier-initialisiertes Netz vortrainiert wird. Hierbei fällt der Validierungsfehler auf CIFAR-10A nach der ersten Epoche von 63 \%, bei reiner Xavier-Initialisierung, auf den besseren Wert von 61.8 \%. Insgesamt werden die Gewichte des \textit{Net-7} bei Xavier-Initialisierung mit $\sigma \approx 0.035$ in den Convolution-, $\sigma \approx 0.18$  im Hidden- und $\sigma \approx 0.16$ im Output-Layer initialisiert. 

Während der Experimente wurde stets eine \textit{Early Stopping Patience} von fünf Epochen verwendet und das Training anschließend mit den gesamten Daten für maximal fünf weitere Epochen fortgesetzt. Dies bedeutet eine starke Beschränkung der Trainingszeit, was sich gerade beim Training mit den CIFAR-10-Daten bemerkbar macht. Hier wirken durch die kurze Trainingszeit die Filtermasken noch recht verrauscht. Trotz der Beschränkung erreicht das \textit{LeNet 5+} einen bemerkenswerten Testfehler von 0.61 \%. Wird die Wartezeit beim \textit{Early Stopping} auf 15 Epochen erhöht, erreicht auch das \textit{Net-7} mit \textit{Padding} und angewandtem Dropout von 0.3 \% im MLP und dem letzten Convolution-Layer nach 72 Epochen einen Testfehler von 25.02 \%. Das stellt im Vergleich zum deutlich größeren Netz von \cite{Masci2011} (100-150-200/300-10) einen plausiblen Wert dar.

Zusammenfassend lassen sich folgende Folgerungen aus den Experimenten ableiten:

\begin{itemize}
\item Die Zentrierung der Daten ist obligatorisch.
\item Padding erhöht die Kapazität und verbessert die Performanz ohne markant größeren Rechenaufwand.
\item Max-Norm-Regularisierung verhindert \textit{Overfitting} zu Beginn des Trainings und beschleunigt letzteres.
\item Dropout und L2-Regularisierung sind gute Verfahren zur Verbesserung der Generalisierung.
\item Die L2-Regularisierung dominiert die Max-Norm-Regularisierung.
\item Die Anwendung einer starken Regularisierung verlängert die Trainingszeit immens.
\item Ein kleiner dimensioniertes Netz kann gleichwertige Ergebnisse wie ein großes, nicht regularisiertes Netz erzielen.
\item Methoden zur Visualisierung sind gute Werkzeuge, um erlernte Merkmale sichtbar zu machen und die Funktionsweise von CNNs zu erklären.
\end{itemize}

Im Rahmen der Experimente kamen bewährte Standardwerte für die Hyperparameter zum Einsatz. Für spezielle Probleme müssen diese durch Kreuzvalidierung oder ähnliche Verfahren bestimmt werden. Ein guter Ausgangspunkt für die Optimierung sind das AdaDelta-Verfahren oder das SGD-ähnliche Nesterov-Momentum ohne adaptive Lernrate, da diese das Training stark beschleunigen können. 
Für die Bestimmung der optimalen Hyperparameter ist die Rechenkapazität eine restriktive Größe. So ist das Training der MNIST-Daten mit \textit{ConvNetCPP} auf dem \textit{LeNet 5+} mit etwa einer Stunde CPU-Zeit pro Epoche noch recht schnell. Auf dem CIFAR-10-Datensatz in Kombination mit dem \textit{Net-7} dauert das Training mit etwa drei Stunden CPU-Zeit pro Epoche jedoch sehr lange. Damit ist das Training größerer Netze, wie des CIFAR-10-Netz von \cite{Masci2011}, sehr zeitaufwändig. Auch sind längere Trainingszeiten von 280 Epochen für das CIFAR-10-Netz von \cite{Zeiler2013b} nur schwer möglich. 
In der Praxis sollte mit einem kleinen Netz gestartet und dieses sukzessive ausgebaut werden, bis der Validierungsfehler nicht weiter sinkt oder der Trainingsfehler den Wert Null erreicht. Anschließend sollten die Fehlerraten von Trainings- und Validierungsdaten mit den beschriebenen Techniken der Regularisierung angeglichen werden, um eine bestmögliche Generalisierung zu ermöglichen.

\section{Ausblick}
Betrachtet man die Laufzeiten der verschiedenen Experimente, wird klar, dass auch ein optimiertes CPU-basiertes CNN für große Datensätze und tiefe Architekturen nicht geeignet ist. Durch die Reduktion der 2D-Faltung auf eine Matrix-Matrix-Multiplikation (GEMM), wird die meiste Rechenzeit für solcherlei Multiplikationen aufgewendet. Diese kann durch eine entsprechende GPU-Implementierung deutlich reduzieren werden. Darüber hinaus lässt sich das Training eines Modells durch spezielle Modell-Parallelisierung ebenfalls beschleunigen \cite[vgl.][]{Krizhevsky2014}. Die Beschleunigungen sind auch hinsichtlich erweiterter Trainingsdaten (\textit{Data Augmentation}) zur Steigerung der Performanz von Bedeutung.

Die Performanz eines CNN lässt sich auch durch weitere spezielle Schichten verbessern. An dieser Stelle sei auf die speziellen \textit{Max-Out}-Netze von \cite{Goodfellow_maxout_2013} und die \textit{DropConnect}-Netze von \cite{Zeiler2013} verwiesen, welche besonders auf dem CIFAR-10-Datensatz hervorragende Ergebnisse liefern.
Weiterhin existieren Schichten, welche nach jedem Convolution-Layer zur lokalen Kontrastnormalisierung eingesetzt werden. Dies verhindert, dass einzelne \textit{Feature-Maps} die Aktivierungen der Schicht dominieren (vgl. \cite{Jarrett2009}).
Darüber hinaus zeigen neuere Netze eine weitere Adaption der üblichen Convolutional-Layer. Meist werden diese als \textit{locally connected} oder LC-Layer bezeichnet (vgl. \cite{Nouri2013} und \cite{LeRanzato2012}). Solche Schichten zeichnen sich dadurch aus, dass lokale rezeptive Felder zwar definiert, die Parameter zwischen den einzelnen Feldern allerdings nicht geteilt werden. Der ursprüngliche Vorteil von CNNs durch \textit{Parameter Sharing} das Modell zu beschränken wird hierdurch verworfen, allerdings kann die größere Kapazität durch anderweitige Regularisierung dennoch zu besseren Ergebnissen führen (vgl. \cite{Hinton2012} und \cite{Krizhevsky2012}).
In diesem Zusammenhang sind auch weitere Experimente im Rahmen des unüberwachten Vortrainings von Interesse, in denen beispielsweise das gesamte CNN mittels eines Stacked Denoising Autoencoders (SDA) initialisiert wird.

Im Allgemeinen ist das Training eines zufällig initialisierten Netzes, beispielsweise auf Basis der ImageNet-Daten \cite[vgl.][]{ImageNet2015}, sehr zeitaufwändig. An dieser Stelle soll deshalb noch ein Hinweis auf eine Ablage bereits trainierter Netze für Caffe gegeben werden \cite[vgl.][]{Caffe2014}, welche für weitere Experimente von Interesse sein kann.\footnote{Projektseite von Caffe: \url{https://github.com/BVLC/caffe} (22.09.2015)}  Diese Netze können beispielsweise zur Merkmalsextraktion oder für Feintuning im Rahmen des Transferlernens eingesetzt werden.

Insgesamt zeigt sich, dass Convolutional Neural Networks in Verbindung mit Methoden des Deep Learning ein ausgezeichnetes Modell für die Klassifikation und Regression von Daten mit lokalen Merkmalen im Eingaberaum darstellen. Insbesondere die Möglichkeit Vorwissen über die Struktur der Daten einzubetten bietet eine große Vielfalt an Optimierungsmöglichkeiten. Dennoch ist die Suche nach den optimalen Hyperparametern schwierig und die Suche nach Fehlern oftmals zeitintensiv. Besonders hier gilt es neue Verfahren zu entwickeln, um die Handhabung solcher Modelle zukünftig einfacher zu gestalten.

